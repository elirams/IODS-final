# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
boston_scaled2 <- scale(Boston)
summary(boston_scaled2)
dist_eu <- dist(boston_scaled2)
km <- kmeans(boston_scaled2, centers = 3)
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y= twcss, geom = 'line')
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled)
dist_eu <- dist(boston_scaled2)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
km <- kmeans(boston_scaled2, centers = 3)
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
km <- kmeans(boston_scaled2, centers = 3)
km <- kmeans(boston_scaled2, centers = 3)
str(Boston)
library(MASS)
str(Boston)
install.packages("installr")
library("MASS", lib.loc="~/R/win-library/3.4")
str(Boston)
dim(Boston)
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot.mixed(cor_matrix, lower.col = "black", number.cex = .6)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
# First we create the quantile vector of crim:
bins <- quantile(boston_scaled$crim)
bins
# Then we create the categorical variable "crime":
crime <- cut(boston_scaled$crim, breaks = bins, inlcude.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
boston_scaled2 <- scale(Boston)
summary(boston_scaled2)
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
sum(row.has.na)
boston_scaled2 <- boston_scaled2[!row.has.na,]
row.has.nan <- apply(boston_scaled2, 1, function(x){any(is.nan(x))})
sum(row.has.nan)
boston_scaled2 <- boston_scaled2[!row.has.nan,]
row.has.inf <- apply(boston_scaled2, 1, function(x){any(is.infinite(x))})
sum(row.has.inf)
boston_scaled2 <- boston_scaled2[!row.has.inf,]
km <- kmeans(boston_scaled2, centers = 3)
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y= twcss, geom = 'line')
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.withinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <- kmeans(boston_scaled2, centers = 2)
pairs(boston_scaled2, col=km$cluster)
km <- kmeans(boston_scaled2, centers = 2)
pairs(boston_scaled2, col=km$cluster)
pairs(boston_scaled2[1:5], col=km$cluster)
pairs(boston_scaled2[6:10], col=km$cluster)
summary(Boston)
boston_scaled3 <- scale(Boston)
summary(boston_scaled3)
km2 <- kmeans(boston_scaled3, center = 3)
boston_scaled3 <- as.data.frame(boston_scaled3)
lda.fit2 <- lda(km2$cluster ~., data=boston_scaled3)
lda.fit2 <- lda(km2$cluster ~., data=boston_scaled3)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(km2$cluster)
# plot the lda results
plot(lda.fit2, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
library("plotly", lib.loc="~/R/win-library/3.4")
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
library("MASS", lib.loc="~/R/win-library/3.4")
str(Boston)
dim(Boston)
library("tidyverse", lib.loc="~/R/win-library/3.4")
library("corrplot", lib.loc="~/R/win-library/3.4")
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot.mixed(cor_matrix, lower.col = "black", number.cex = .6)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
# First we create the quantile vector of crim:
bins <- quantile(boston_scaled$crim)
bins
# Then we create the categorical variable "crime":
crime <- cut(boston_scaled$crim, breaks = bins, inlcude.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
boston_scaled2 <- scale(Boston)
summary(boston_scaled2)
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
km <- kmeans(boston_scaled2, centers = 3)
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.withinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <- kmeans(boston_scaled2, centers = 2)
pairs(boston_scaled2, col=km$cluster)
pairs(boston_scaled2[1:5], col=km$cluster)
pairs(boston_scaled2[6:10], col=km$cluster)
summary(Boston)
boston_scaled3 <- scale(Boston)
summary(boston_scaled3)
km2 <- kmeans(boston_scaled3, center = 3)
boston_scaled3 <- as.data.frame(boston_scaled3)
lda.fit2 <- lda(km2$cluster ~., data=boston_scaled3)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(km2$cluster)
# plot the lda results
plot(lda.fit2, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
library("plotly", lib.loc="~/R/win-library/3.4")
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km2$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep = ",", col.names = TRUE, row.names = TRUE)
install.packages("installr")
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep = ",", col.names = TRUE, row.names = TRUE)
library("dplyr", lib.loc="~/R/win-library/3.4")
library("FactoMineR", lib.loc="~/R/win-library/3.4")
library("ggplot2", lib.loc="~/R/win-library/3.4")
library("tidyr", lib.loc="~/R/win-library/3.4")
library("corrplot", lib.loc="~/R/win-library/3.4")
library("magrittr", lib.loc="~/R/win-library/3.4")
library("magrittr", lib.loc="~/R/win-library/3.4")
library("dplyr", lib.loc="~/R/win-library/3.4")
library("magrittr", lib.loc="~/R/win-library/3.4")
library("dplyr", lib.loc="~/R/win-library/3.4")
library("corrplot", lib.loc="~/R/win-library/3.4")
cor(human)
human <- read.table(file="C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-project/IODS-project/Data/human.csv", header = TRUE, row.names = 1)
str(human)
str(human)
dim(human)
summary(human)
library(GGally)
ggpairs(human)
cor(human)
corrplot(human)
humancor <- cor(human)
corrplot(humancor)
pca_human <- prcomp(human)
s <- summary(pca_human)
s
pca_pr <- round(100*s$importance[2,], digits=1)
pca_pr
biplot(pca_human, choices = 1:2, cex = c(0.6, 1))
human <- scale(human)
pca_human <- prcomp(human)
s <- summary(pca_human)
s
pca_pr <- round(100*s$importance[2,], digits=1)
pca_pr
biplot(pca_human, choices = 1:2, cex = c(0.6, 0.9))
pca_lab <- paste0(names(pca_pr), "(", pca_pr, "%)")
biplot(pca_human, cex = c(0.6, 1), col=c("grey40", "deeppink2"), xlab= pca_lab[1], ylab = pca_lab[2])
library(FactoMineR)
data("tea")
str(tea)
dim(tea)
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- select(tea, one_of(keep_columns))
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
mca <- MCA(tea_time, graph = FALSE)
summary(mca)
plot(mca, invisible = "ind", habillage = "quali")
plot(mca, invisible = "var", habillage = "quali")
library("ggplot2", lib.loc="~/R/win-library/3.4")
library("tidyr", lib.loc="~/R/win-library/3.4")
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- select(tea, one_of(keep_columns))
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
?select
?render
??render
install.packages(c("knitr", "rmarkdown"))
install.packages(c("knitr", "rmarkdown"))
human <- read.table(file="human.csv", header = TRUE, row.names = 1)
str(human)
human <- read.table(file="C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-project/IODS-project/Data/human.csv", header = TRUE, row.names = 1)
str(human)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
dim(hd)
summary(hd)
str(gii)
dim(gii)
summary(gii)
colnames(hd)[1] <- "HDI.rank"
colnames(hd)[2] <- "country"
colnames(hd)[3] <- "HDI"
colnames(hd)[4] <- "lifexp"
colnames(hd)[5] <- "yr.eduexp"
colnames(hd)[6] <- "meanyr.edu"
colnames(hd)[7] <- "GNI"
colnames(hd)[8] <- "GNIrank-HDIrank"
colnames(gii)[1] <- "GII.rank"
colnames(gii)[2] <- "country"
colnames(gii)[3] <- "GII"
colnames(gii)[4] <- "matermor"
colnames(gii)[5] <- "adobirth"
colnames(gii)[6] <- "repreparl"
colnames(gii)[7] <- "edu2F"
colnames(gii)[8] <- "edu2M"
colnames(gii)[9] <- "labforceF"
colnames(gii)[10] <- "labforceM"
colnames(hd)
colnames(gii)
gii <- mutate(gii, edu2.mean = (gii$edu2F+gii$edu2M)/2)
library("dplyr", lib.loc="~/R/win-library/3.4")
gii <- mutate(gii, edu2.mean = (gii$edu2F+gii$edu2M)/2)
colnames(gii)
summary(gii$edu2.mean)
library(dplyr)
human <- inner_join(hd, gii, by = "country")
glimpse(human)
library(stringr)
human <- mutate(human, GNI = str_replace(human$GNI, pattern = ",", replace = "") %>% as.numeric())
human$GNI
keep <- c("country", "yr.eduexp", "meanyr.edu", "edu2F", "edu2.mean" "lifexp", "matermor", "adobirth", "GNI")
keep <- c("country", "yr.eduexp", "meanyr.edu", "edu2F", "edu2.mean", "lifexp", "matermor", "adobirth", "GNI")
human <- select(human, one_of(keep))
complete.cases(human)
data.frame(human, comp = complete.cases(human))
human_ <- filter(human, complete.cases(human))
str(human_)
tail(human_, n= 10)
last <- nrow(human_) - 7
human_ <- human_[1:last,]
rownames(human_) <- human_$country
human_ <- select(human_, -country)
str(human_)
glimpse(human_)
setwd("C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-final/")
write.table(human_, file = "human.csv", sep = " ", col.names = TRUE, row.names = TRUE)
human <- read.table(file="human.csv", header = TRUE, row.names = 1)
str(human)
human <- read.table(file="C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-final/human.csv", header = TRUE, row.names = 1)
str(human)
knitr::opts_chunk$set(echo = TRUE)
human <- read.table(file="C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-final/human.csv", header = TRUE, row.names = 1)
str(human)
install.packages("kable")
Label<-c("yr.eduexp", "meanyr.edu", "edu2F", "edu2.mean", "lifexp", "matermor", "adobirth", "GNI")
Variable<-c("Expected years of schooling", "Mean years of schooling", "Female population with at least secondary education, in %", "Population with at least secondary school, in %", "Life expectancy at birth, in years", "Maternal mortality ratio(deaths per 100 000 births)", "Adolescent birth rate (births per 1 000 women ages 15-19)", "Gross national income (GNI) per capita, in USD")
variables<- data.frame(Label,Variable)
kable(variables, "html") %>%
kable_styling(bootstrap_options = "striped", full_width = F)
install.packages("kable")
library(kable)
install.packages("knitr::kable")
installed.packages("installr")
install.packages("installr")
install.packages("installr")
install.packages("knitr::kable")
install.packages("knitr::kable")
library(kable)
df_print(human)
print(human)
print.data.frame(human)
?print.data.frame
Variable<-c("Expected years of schooling", "Mean years of schooling", "Female population with at least secondary education, in %", "Population with at least secondary school, in %", "Life expectancy at birth, in years", "Maternal mortality ratio(deaths per 100 000 births)", "Adolescent birth rate (births per 1 000 women ages 15-19)", "Gross national income (GNI) per capita, in USD")
Label<-c("yr.eduexp", "meanyr.edu", "edu2F", "edu2.mean", "lifexp", "matermor", "adobirth", "GNI")
variables<- data.frame(Label,Variable)
print.data.frame(variables)
summary(human)
gather(human) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
library("dplyr", lib.loc="~/R/win-library/3.4")
library("ggplot2", lib.loc="~/R/win-library/3.4")
gather(human) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
library("tidyverse", lib.loc="~/R/win-library/3.4")
gather(human) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
ggpairs(human, mapping=aes(), lower=list(combo=wrap("facethist", bins = 20)))
library("GGally", lib.loc="~/R/win-library/3.4")
ggpairs(human, mapping=aes(), lower=list(combo=wrap("facethist", bins = 20)))
library("corrplot", lib.loc="~/R/win-library/3.4")
cor(human)%>%corrplot()
cor(human)%>%corrplot.mixed(lower.col = "black", number.cex = .6)
cor(human)%>%corrplot.mixed(lower.col = "black", number.cex = .8)
cor(human)%>%corrplot.mixed(number.cex = .8)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.srt = 60)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.srt = 50)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.srt = 40)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.srt = 80)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.srt = 50)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.col = black)
cor(human)%>%corrplot.mixed(number.cex = .8, tl.col = "black")
cor(human)%>%corrplot.mixed(number.cex = .8, tl.col = "black", order = "hclust")
cor(human)%>%corrplot.mixed(number.cex = .8, tl.col = "black", order = "hclust")
cor(human)%>%corrplot.mixed(number.cex = .8, tl.col = "black")
par(mfrow=c(2,4))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
par(mfrow=c(2,4))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_boxplot() + xlab("Years of expected education") + ylab("Life expectancy")
par(mfrow=c(2,4))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
par(mfrow=c(2,4))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_point() + xlab("Years of expected education") + ylab("Life expectancy")
par(mfrow=c(2,4))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
par(mfrow=c(1,2))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
g2 <- ggplot(human, aes(x=meanyr.edu, y=matermor))
g2 + geom_smooth() + xlab("Mean years of education") + ylab("Maternal mortality")
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
g2 <- ggplot(human, aes(x=meanyr.edu, y=matermor))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
g2 <- ggplot(human, aes(x=meanyr.edu, y=matermor))
g2 + geom_smooth() + xlab("Mean years of education") + ylab("Maternal mortality")
par(c(g1, g2), mfrow=c(1,2))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
g2 <- ggplot(human, aes(x=meanyr.edu, y=matermor))
g2 + geom_point() + xlab("Mean years of education") + ylab("Maternal mortality")
par(c(g1, g2), mfrow=c(1,2))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_smooth() + xlab("Years of expected education") + ylab("Life expectancy")
g2 <- ggplot(human, aes(x=meanyr.edu, y=matermor))
g2 + geom_smooth() + xlab("Mean years of education") + ylab("Maternal mortality")
par(c(g1, g2), mfrow=c(1,2))
g1 <- ggplot(human, aes(x=yr.eduexp, y=lifexp))
g1 + geom_point() + xlab("Years of expected education") + ylab("Life expectancy")
g2 <- ggplot(human, aes(x=meanyr.edu, y=matermor))
g2 + geom_point() + xlab("Mean years of education") + ylab("Maternal mortality")
par(c(g1, g2), mfrow=c(1,2))
